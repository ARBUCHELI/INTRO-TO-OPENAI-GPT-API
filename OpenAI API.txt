# OPENAI API
------------

* Introduction
--------------
An Application Programming Interface (API) is an excellent way for developers to interact with third-party software easily. Data and functionality can be accessed by sending requests to 
different API endpoints. Using APIs is a great way to expand your project’s scope and capabilities.

The OpenAI API is no different because it is a portal into the world of large language models and brings generative AI into our code. Understanding this API is important when working with 
generative AI in your code and projects.

Generative AI applications like ChatGPT use large language models (LLMs). LLMs, in turn, are built using a technology known as Generative Pre-trained Transformer (and they are named after 
it, hence GPT-4). A main driver of these models is the Neural Network. Based loosely on the human brain’s functionality, neural networks are trained on a large amount of data to reproduce 
certain behaviors.

While powerful and very effective, neural networks operate as a black box because their inner workings are a mystery to even the most highly-trained data scientists. This results in 
unverifiable model output, where the input and outputs are always processed with a certain amount of randomness.

This lesson will examine the OpenAI API and explain how to create more reliable, controlled outputs from LLMs. We will explore the API’s different generative AI tasks and model offerings, 
examine the API request and response objects, and look at effectively creating input prompts to receive the desired text output. We will also look at controlling the output with 
hyperparameters to counteract the model’s non-deterministic or random behavior.

* Instructions
--------------
The GPT simulator on the right lets you input specific prompts and get a corresponding output. To do this, select a preset prompt by clicking the button labeled with the prompt text. 
Then, press the triangle button to the right to submit it. If you wish to remove all output text, click the circular arrow button located next to the triangle button.

Try submitting each prompt multiple times and observe the output.

You’ll see that depending on the input prompt, the output might be less deterministic and differ with each submission. Using the API, you will learn how to gain some control of the output 
based on the given input.

When you’re done, move to the next exercise.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

* Endpoints
-----------
The OpenAI API can be used for text completion, image generation and modification, audio transcription, and more. Text completion, including code generation, is a popular use case for 
large language models and is what we will focus on in this course.

API endpoints are server locations where data can be sent and received using code or other means. OpenAI’s API has two endpoints that handle text completion: chat/completions and 
completions. Let’s compare these two text completion endpoints.

. chat/completions
------------------

The chat/completions endpoint generates text from a single prompt or defines a multi-prompt chat.

The chat functionality allows us first to set the “behavior” of the chat replies, such as “You are to respond to 8th-grade social studies questions.” Once this is defined, the responses 
will be on an 8th-grade social studies class level, and questions not involving social studies will potentially be rejected.

Using this endpoint, we can also specify a small number of prompt/response examples to help guide future responses. This is known as few-shot prompting and can profoundly affect the 
chat’s behavior.

. completions
-------------
The completions endpoint produces a text completion output using a single input prompt. While this behavior is also available in the chat/completions, the completions endpoint does 
provide more hyperparameters for tuning the model’s behavior which could be useful for some developers.

However, despite offering those extra hyperparameters that aren’t available in chat/completions, the completions endpoint is not as useful overall since it only supports a single prompt 
for completion and uses older LLMs that are less powerful and cost more.

. Other Endpoints
-----------------
While we will be focusing on the chat/completions and completions endpoints in this course, it is good to know about some of the other endpoints accessible with the API:

	. models: List the available models and corresponding data such as owner and permissions
	. images: Create and edit images using a text prompt
	. audio: Generate text from a given audio file
	. moderations: Analyze text to see if it violates OpenAI’s content policy, which is useful when passing a user’s input through to the API

. Instructions
--------------
Click the generative AI categories on the right to reveal the associated endpoint and description.

- Chat and Text Completion Endpoint 
-----------------------------------
'chat/completions'

Generates and completes text from a single prompt or defines a multi-prompt chat.

- Text Completion Endpoint
--------------------------
'completions'
Generates and completes text from a single prompt.

- Available Models Endpoint
---------------------------
'models'
List the available models and corresponding data.

- Image Creation Endpoint
-------------------------
'images'
Create and edit images using a text prompt.

- Audio Transcription Endpoint
------------------------------
'audio'
Generates text from a given audio file

- Content Moderation Endpoint
-----------------------------
'moderations'
Analyzes text to see if it violates OpenAI's content policy.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Models, Tokens and Cost
-------------------------
There are many different LLMs available through the OpenAI API. Each model has varying performance and capabilities and can only be accessed through certain API endpoints.

The newer models, such as GPT-3.5 and GPT-4 are available through the chat/completions endpoint, and older models, like text-davinci-002 and text-davinci-003 are available through the 
completions endpoint.

There are many aspects to the different models available to the API, but one very important measurement is the number of parameters in a model. Not to be confused with hyperparameters, a 
model’s parameters are the tuned values used by the neural nets in the large language model that allow the model to process the input and generate the output. If you like to think of 
things in terms of math, you can think of these parameters as weighted coefficients.

OpenAI has hinted that GPT-4 has over 1 trillion parameters, which makes it way bigger than GPT-3.5’s 175 billion parameters.

* Tokens
--------
Tokens are words, or pieces of words, and are what LLMs use to parse the given input and generate an output. Each model has a maximum number of tokens it can process, and the cost of 
using each model is based on the number of tokens in the input prompt and the generated output.

A general rule is that about 750 words equals 1000 tokens. When picking what OpenAI model to use, the cost is given in an amount per 1000 tokens. The most cost-effective model is 
GPT-3.5-turbo, available through the chat/completions endpoint.

* Cost Analysis
---------------
A less expensive model, such as GPT-3.5-turbo, might require more generations of text completions to get close to a desired output. Sometimes, using the costly model will require fewer 
generations to reach that final product. It might also be the case that using any GenAI at all is more expensive than hiring people to accomplish the same task. This cost analysis is 
essential to choose the best path forward.

* Instructions
--------------
To the right is a table detailing several common text completion models, maximum tokens, costs, and endpoints. There are a few things to note regarding this information:

	. OpenAI is always updating the costs of their models. It is important to reference openai.com/pricing to get updated costs.

	. gpt-3.5-turbo is by far the most cost-efficient model. While it does not have as many parameters as gpt-4, it is considered the best price to performance when using the 
	chat/completions endpoint.
	
	. The text-davinci-003 model is this table’s only completions endpoint model. It is more expensive than gpt-3.5-turbo and has already been marked as “legacy”. This means it will 
	be replaced with a more efficient model in the near future.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

* Input Prompt
--------------
LLMs are known for generating text, but it is important to remember that they have to parse the input prompt text to generate the output. The text completion endpoints have different 
specifications regarding the formats of the input prompts they require:

	. The completions endpoint requires a single input string saved to the prompt parameter.

	. The chat/completions endpoint differs because it allows us to send multiple inputs within the messages parameter. This allows us to set up the chat functionality and perform 
	few-shot prompting.

* Chat Roles
------------
For the chat/completions endpoint, each input requires information on the input’s role and content, where role defines the purpose of the given content. The four roles in the 
chat/completions prompt are:

	. system: Used once and defines the chatbot’s behavior. This can guide the output toward certain topics.
	---------

	. user: This role can be used independently as a single prompt similar to the completions endpoint behavior. It can also be used in few-shot prompting. When a user prompt is 
	-------
	paired with an assistant prompt, we are giving chat examples and defining how the chat should respond in the future.

	. assistant: Used with the user role to give an example reply to a preceding user input. This is useful for few-shot prompting.
	------------

	. function: This is a more advanced role to format output for specified function parameters.
	-----------

* Prompt Engineering
--------------------
A well-designed input prompt must be descriptive while efficiently using tokens, whether creating a single input prompt with either endpoint or using few-shot prompting with the chat 
endpoint. Crafting input prompts to get the desired output is known as prompt engineering. The following are a few strategies to create good prompts:

	. Be Descriptive: Use adjectives and descriptive language to give the model more context on what to output.
	-----------------

	. Be Specific: Avoid generalizing with words like “a few” and instead use “three”.
	--------------

	. Define the Output: Ask for the output to be in a certain format, such as JSON or listed out.
	--------------------

	. Provide an Example: If you have an example of the end result you’re looking for, include it in your prompt.
	---------------------

------------------------------------------------------------------------------------------------------------------------------------------------------------------



























































